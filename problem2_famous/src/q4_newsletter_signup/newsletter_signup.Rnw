\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{fullpage}
\usepackage{pdflscape}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{datetime}
\usepackage{url}
\usepackage{listings}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
\flushbottom

<<setup, include=FALSE, cache=FALSE>>=
rm(list=setdiff(ls(), "visitData"))   #clear memory
library(knitr)
library(plyr) #load packages already to abort early if not installed
library(doMC)
library(jsonlite)
library(Deducer)
# set global chunk options
opts_chunk$set(fig.path='figure/', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\date{\currenttime, \today}
\title{Almost Famous: Analyse Newsletter Signup Rate Per Experiment}
\author{Cindy Lamm}

\maketitle

% ------------------------
% ------------------------ define functions
% ------------------------

<<define-get-pattern-index, include=FALSE>>=
source("../getPatternIndex.R")
@

<<define-aggregate-per-experiment, include=FALSE>>=
library(plyr)
library(doMC)
registerDoMC(cores=detectCores())
aggregatePerExperiment12 <- function(df) {
   agg <- ddply(df, .(experiment_12), summarise,
                nb_visits=length(unique(visit_id)),
                nb_uids=length(unique(uid)),
                total_signups=sum(nb_signups),
                total_non_signups=nb_uids-total_signups,
                signup_rate=total_signups/nb_uids,
                .parallel=TRUE)                  
   return(agg)
}

aggregatePerExperiment34 <- function(df) {
   agg <- ddply(df, .(experiment_34), summarise,
                nb_visits=length(unique(visit_id)),
                nb_uids=length(unique(uid)),
                total_signups=sum(nb_signups),
                total_non_signups=nb_uids-total_signups,
                signup_rate=total_signups/nb_uids,
                .parallel=TRUE)                  
   return(agg)
}
@

% ------------------------
% ------------------------ web analysis starts here
% ------------------------

Load variable names and types:
<<load-meta-data>>=
nameTypeDataFile  <- "../../data/raw_variables.csv"
variableNames <- read.csv(nameTypeDataFile, header=TRUE, stringsAsFactors=FALSE) 
variableNames
factorIdx <- which(variableNames$type=="factor")
factorNames <- variableNames$name[factorIdx]
@

Read the per visit aggregated web log data:
<<read-web-data>>=
visitFile <- "../../data/web_visits.csv"
visitData <- read.csv(visitFile, stringsAsFactors=FALSE, col.names=variableNames$name, 
                      colClasses=variableNames$type, na.strings=c("NA",""))
visitData$tstamp <- as.POSIXct(visitData$tstamp)
str(visitData)
@

\pagebreak 

<<summary-visit-data>>=
summary(visitData)
@

\section{Newsletter Signup Rate Per Experiment}

What are the actions per visit??
<<table-actions>>=
table(visitData$action)
@

Look at visits with signups:
<<get-visits-with-signups>>=
signupIdx <- getPatternIndex(visitData$action, "signup")
totalSignups <- length(signupIdx)
@

I conclude from the factor levels for \verb+action+ that there is at most 1 signup per visit and overall \Sexpr{totalSignups} signups. I cross check with a simple grep on the command line on the unaggregated web data which gives us the same result:

\begin{lstlisting}
$ grep -o signup web.log | wc -l
$ 89352
\end{lstlisting}

Add the number of signups per visit as variable to the data frame:
<<add-nb-signups-to-df>>=
nbSignup <- rep(0, nrow(visitData))
nbSignup[signupIdx] <- 1
visitData$nb_signups <- nbSignup
@

<<prop-table-of-nb-signups, include=FALSE>>==
ptab <- prop.table(table(visitData$nb_signups))
@

There are $\Sexpr{round(ptab[1]*100,2)}\%$ of visits that don't have a signup and only $\Sexpr{round(ptab[2]*100,2)}\%$ that do.

Checkout experiment information:
<<table-experiments-combinations>>=
prop.table(table(visitData$experiments))
@

Split up the experiment information into separate variables
<<split-experiments-into-separate-variables>>=
expIdx1 <- getPatternIndex(visitData$experiments, 1)
totalExp1 <- length(expIdx1)
expIdx2 <- getPatternIndex(visitData$experiments, 2)
totalExp2 <- length(expIdx2)
expIdx3 <- getPatternIndex(visitData$experiments, 3)
totalExp3 <- length(expIdx3)
expIdx4 <- getPatternIndex(visitData$experiments, 4)
totalExp4 <- length(expIdx4)
@

and add them pairwise to the data frame:
<<add-nb-separate-experiment-variables-to-df>>=
stopifnot(!any(intersect(expIdx1, expIdx2)), 
          totalExp1 + totalExp2 == nrow(visitData), 
          !any(intersect(expIdx3, expIdx4)),
          totalExp3 + totalExp4 == nrow(visitData))

experiment12 <- rep(1, nrow(visitData))
experiment12[expIdx2] <- 2
visitData$experiment_12 <- factor(experiment12, levels=1:2)

experiment34 <- rep(3, nrow(visitData))
experiment34[expIdx4] <- 4
visitData$experiment_34 <- factor(experiment34, levels=3:4)
@

Checkout experiment distribution:
<<distribution-of-experiments>>=
prop.table(table(visitData$experiment_12))
prop.table(table(visitData$experiment_34))
@


How many signups are there per experiment?
<<summary-per-experiment>>=
visitAggExp12 <- aggregatePerExperiment12(visitData)
visitAggExp12

visitAggExp34 <- aggregatePerExperiment34(visitData)
visitAggExp34
@

Write the result into json file:
<<write-output-to-json>>=
library(jsonlite)
overallSignupRates <- c(visitAggExp12$signup_rate, visitAggExp34$signup_rate)
names(overallSignupRates) <- paste("experiment", 1:4, sep="")
jsonString <- toJSON(as.data.frame(t(overallSignupRates)), dataframe="rows", pretty=TRUE)
jsonString
write(jsonString, file="../q4_newsletter_signup/out/overallSignupRates.json")
@

\section{Performance Of Experiments}

I assume that the performance of an experiment is measured by the number of signups for the newsletter. 
<<table-signups-per-experiment>>=
gTestTable12 <- cbind(visitAggExp12$total_signups, visitAggExp12$total_non_signups)
rownames(gTestTable12) <- paste("Experiment",1:2,sep="")
colnames(gTestTable12) <- c("signups","ignorations")
gTestTable12

gTestTable34 <- cbind(visitAggExp34$total_signups, visitAggExp34$total_non_signups)
rownames(gTestTable34) <- paste("Experiment",3:4,sep="")
colnames(gTestTable34) <- c("signups","ignorations")
gTestTable34
@

<<save-gtesttable, include=FALSE>>=
# Save the input table because Deducer library can't be loaded in RStudio on Mac
# but it can beloaded on the command line
save(gTestTable12, file="../q4_newsletter_signup/gTestTable12.RData")
save(gTestTable34, file="../q4_newsletter_signup/gTestTable34.RData")
@

<<set-alpha, include=FALSE>>=
ALPHA=0.001
@


Run G-Test, which is a test of independence (just like the Chi-Square test). The null hypothesis is that the two variables are independent, which is rejected if the p-value is smaller than a chosen significance level $\alpha$. Usually one uses $\alpha=5\%$ but with such a large sample size I would opt for $\alpha=\Sexpr{ALPHA}$, which means that if we decide to reject the null hypothesis and assume the variables are dependent (i.e. in this case this means one experiment is better than the other), we might be wrong for $\alpha \times n=\Sexpr{round(ALPHA*sum(visitAggExp12$nb_uids))}$ observations.

<<run-g-tests>>==
library(Deducer)
test12 <- likelihood.test(gTestTable12)
print(test12)
# sampleSize
sum(visitAggExp12$nb_uids)

test34 <- likelihood.test(gTestTable34)
print(test34)
#sample size
sum(visitAggExp34$nb_uids)
@

I would interpret the results as follows: Experiments 1 and 2 are indistinguishable based on the number of signups - it doesn't matter whether the color scheme of the page is blue or green.

However there is a statistically significant difference between Experiment 3 and 4 - based on the number of newsletter signups the promotional blurb by Josh Willis performs better.

\end{document}