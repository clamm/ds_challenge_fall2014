\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\definecolor{linkcolor}{rgb}{0.686,0.059,0.569}

\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{datetime}
\usepackage{fancyvrb}
\usepackage{dsfont}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true,linkcolor=linkcolor]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\VerbatimFootnotes

\author{Cindy Lamm}
\title{Solution Abstract Problem 2: Almost Famous}

\maketitle


\section{Approach} % (fold)
\label{approach}

I identified question 1 as classification problem (supervised or unsupervised) which I wanted to solve using Spark (after the performance problems I encountered for the classification in R with 7.3 million observations). 

I opted for the following approach to solve the questions:
\begin{itemize}
	\item analyse file structure \& convert to csv
	\item explore data
	\item prepare data for modeling
	\item estimate and validate models for spam classification
	\item prepare the web data for prediction
	\item classify entries in web data as spam
	\item remove spam entries from web data
	\item calculate required metrics on spam free web data
\end{itemize}

% section approach (end)


\subsection{Analyse file structure} % (fold)
\label{sub:analyse_file_structure}
I had a short look into the log data in json format via \href{http://stedolan.github.io/jq/}{jq}. Once I saw that the data is fairly regular (same names for keys, same number of keys per row even if different row "types") I converted the data to csv\footnote{see \verb+problem2_famous/data/convert2csv.sh+} using \href{https://github.com/zemirco/json2csv}{json2csv} (which is pre-installed in \href{http://datasciencetoolbox.org/}{the data science toolbox}) to load it into R for exploratory analysis.

% subsection analyse_file_structure (end)


\subsection{Explore data} % (fold)
\label{sub:explore_data}
To get an impression of the web and spam data I had a look at overall summary statistics but also at summary statistics aggregated over visits and visitors. Since I am not that familiar with web analytics I assume that a \verb+uid+ uniquely identifies a visitor (which I will use as a synonym for user). 

From manual inspection it seems that visits that are generated via the bot network only contain the action \verb+adclick+ after they landed on the page. However if I would classify the web data according to this simple rule, then the click through rate for ads on the page would be zero - which seems too extreme.

% subsection explore_data (end)

\subsection{Prepare data for modeling} % (fold)
\label{sub:prepare_data_for_modeling}
In order to run a classification/clustering algorithm in Spark I need to code the categorical variables (all but \verb+tstamp+) with integer levels (which means that I implicitely assume that it is okay if an algorithm treats these variables as continuous features, because the resulting model is still good enough).
I converted the \verb+tstamp+ to epoch values (i.e. to seconds since 1970-01-01 00:00:00 GMT) to have a true continuous variable without loosing the time information.

For the supervised classification I added a variable \verb+is_spam+ with value $1$ to the spam data and with value 0 to the web data (knowing that with this assumption some web data is falsely labeled). I pooled the spam data and different amounts of the web data into a training data sets - which I used as well for testing (see section \ref{sub:current_limitations}).

% subsection prepare_data_for_modeling (end)

\subsection{Estimate and validate models for spam classification} % (fold)
\label{sub:estimate_and_validate_models_for_spam_classification}
I tried different classification and cluster algorithms: I started with unsupervised clustering using k-means on a training sample with all spam log data and (the top) $200.000$ observations of the web log data\footnote{Using random $200.000$ observations from the web log data didn't seem to be a good idea given that entries are ordered by time.}. I had the hope that by using $k=2$ all spam data would end up in one cluster and all web data in the other, so that when I predict the complete web data on this clustering, all so far unknown spam entries go into the spam-cluster. However when I used the estimated cluster centers to see the division into clusters of the training data, the result was not as good as I would have expected:

\textcolor{red}{TODO: add venn diagram here or error/accuracy values}

% subsection estimate_and_validate_models_for_spam_classification (end)

\subsection{Current Limitations Of Spam Classification} % (fold)
\label{sub:current_limitations}

I used the same data for training, validation and prediction, which decreases the generalization performance of the estiamted model. It would be better to split the available spam and web log data into independent sets for training and validation.

Given that I relabeled the categorical variables with (rather small) integer labels it might be necessary to scale the epoch values, because they otherwise bias the differences. It might also be better to use 1-of-n encoding for the categorical variables instead of just giving them numerical labels.


Since the results are not as expected I would look for better input features, e.g. the time difference between the actions and/or the total time spent per visit. I also deem worth trying to cluster/classify on aggregated visit and/or user data. %(I didn't do this because of missing compute power in R and missing skills in Spark)

% subsection current_limitations (end)

\subsection{Compute Required Metrics} % (fold)
\label{sub:compute_metrics}

Even if I was not able so far to properly classifiy the spam I prepared all the code to compute the required metrics.

\subsubsection{Overall Click-Trough Rate} % (fold)
\label{subsub:ctr}

I assume the following formula for the overall click-through rate ($CTR$), that was described as "ad clicks per visit"
$$
CTR = \frac{ \#\{action=adclick\} }{ \#\{unique \, visit \, ids\}}
$$

Without having removed the spam visits from the web log data I counted \textcolor{black}{103896} ad clicks and \textcolor{black}{1482602} unique visit ids, which leads to a CTR of \textcolor{black}{$7.01\%$} - which is unusually high (if we believe in wikipedia\footnote{\url{http://en.wikipedia.org/wiki/Click-through_rate}}) and certainly prooves the necessity to exclude the spam entries from the calculation.

% \textbf{After} having removed the spam visits from the web log data I counted \textcolor{red}{xx} ad clicks and \textcolor{red}{xx} unique visit ids, which leads to a CTR of \textcolor{red}{$xx\%$}.

% subsubsection ctr (end)


\subsubsection{Mean Orders Per Visit Within Campaign-Query Combinations} % (fold)
\label{subsub:mean_cq}
Since the action "order" is only listed in log entries that don't have query and campaign information, the first step I did was to aggregate the web log per visit 
%(using Spark \verb+reduceByKey+ method on the key (\verb+visit_id+,\verb+uid+)\,)
putting all actions in a list.
I saved the aggregated visits data back to a log file, converted it to csv and worked on that in R to calculate the mean and standard deviation of number of orders within the groups formed by the campaign-query combination (using the \href{http://www.inside-r.org/packages/cran/plyr/docs/ddply}{ddply} function of package \href{http://cran.r-project.org/web/packages/plyr/index.html}{plyr}).

\textbf{Note:} Of the 50 possible campaign-query combinations only 10 have any entries in the web.log data. So I only wrote the standard deviations of the number of orders per campaign-query combination that have a non-zero value. 
% subsubsection mean_cq (end)


\subsubsection{Newsletter Signup Rate} % (fold)
\label{subsub:signup}

Before I compute the newsletter signup rate for each experiment I double-check the assumption that users see experiments independently by verifying that the distribution of experiments in the data is even\footnote{see \verb+problem2_almost/src/q4a_newsletter_signup/newsletter_signup.pdf+}.

I used the per visit aggregated web log data and computed in R the newsletter signup rate per experiment using the again the ddply function.
% subsubsection signup (end)


\subsubsection{Experiment Performance} % (fold)
\label{subsub:performance}

I assume that the performance of an experiment is measured by the number of signups for the newsletter. Furthermore, from the description "There are no preexisting expectations that either experiment in either pair is more likely to be effective than the other." I conclude that we do a two-sided test.

Note: The G-Test is a test of independence just like the Chi-Square test. The null hypothesis is that the two variables are independent, which is rejected if the p-value is smaller than a chosen significance level $\alpha$. In this case, rejecting the null hypothesis means that one experiment performs better than the other.
Usually one uses $\alpha=5\%$ but with such a large sample size of $n=xx$ I would opt for $\alpha=0.001$, which means that if we decide to reject the null hypothesis, we might be wrong for $\alpha \times n=xx$ observations.

I would interpret the results as follows: 
\begin{itemize}
	\item With a p-value of $p_{12}=xx$ experiments 1 and 2 are indistinguishable - it doesn't matter for the newsletter signup whether the color scheme of the page is blue or green.
	\item With a p-value of $p_{34}=xx$ there is a statistically significant difference between experiment 3 and 4 - based on the number of newsletter signups the promotional blurb by Josh Willis performs better.	 
\end{itemize}
% subsubsection performance (end)


\subsubsection{Required Days for $99\%$ Confidence in G-Test Result} % (fold)
\label{subsub:days}

\begin{quote}
How many full days of data, starting from the first day, are required to determine that the newsletter signup rate for experiment one is better than experiment two at the 99\% confidence level?
\end{quote}

% subsubsection performance (end)


% \subsubsection{Possible Improvements} % (fold)
% \label{subsub:improvements}

% I wrote a separate Spark Python script for each of the metrics I computed with Spark, where I loaded the data from the file system in each script. This could be improved by calculating all metrics within one Spark session for which I would load the data only once from the file system (but need to refactor the code to make it reusable). 
% % subsubsection improvements (end)


% subsection compute_metrics (end)


\section{Used Software}
\label{used_software}
In general I developed on a Macbook Pro with a 2.3 GHz Intel Core i7 Processor and 16GB Memory.

\begin{itemize}
	\item git
	\item jq
	\item json2csv (pre-installed in the data science toolbox)
	\item LateX
	\item R in RStudio with packages Deducer\footnote{only for G-test function \href{http://www.rforge.net/doc/packages/Deducer/likelihood.test.html}{likelihood.test}}, doMC, jsonlite, knitr, plyr 
	\item shell command line
	\item Spark with Python API and MLlib package
\end{itemize}

\section{Time Spent}
\begin{itemize}
	\item 24h for data cleaning, preparing for and running the spam classification
	\item 12h for computing CTR and metrics for campaign-query combinations
	\item 1h for solution abstract
\end{itemize}

Summing up I spend \textcolor{red}{xx hours} on this problem.


\end{document}