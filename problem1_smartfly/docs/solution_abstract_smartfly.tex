\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\definecolor{linkcolor}{rgb}{0.686,0.059,0.569}

\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{datetime}
\usepackage{fancyvrb}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true,linkcolor=linkcolor]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\VerbatimFootnotes

\author{Cindy Lamm}
\title{Solution Abstract Problem 1: SmartFly}

\maketitle


\section{Approach} % (fold)
\label{approach}

I identified the given problem as a classification, respectively a class probability estimation and opted for the following steps as general approach:
\begin{itemize}
	\item analyse file structure
	\item explore the historic and scheduled data
	\item prepare the historic data for modeling
	\item estimate \textcolor{black}{and validate} a model
	\item prepare the scheduled data for prediction
	\item predict the delay probability of scheduled flights using the estimated model
\end{itemize}

% I considered the following general models to do class probability estimation:
% \begin{itemize}
% 	\item Decision/classification Trees and Random Forests
% 	\item Linear Discriminant Analysis / Support Vector Machines
% 	\item k-NN Clustering
% 	\item Logistic Regression
% 	\item Neural Networks
% \end{itemize}


In the spirit of the Lean Startup by Eric Ries, I tried to keep my mind focused on the "minimal viable solution": I first estimated a basic model on a minor set of data without any validation to get as quick as possible to the prediction stage producing the required output file of ordered flight IDs. I then iterated over each step and improved the solution.



\subsection{Analyse file structure} % (fold)
\label{sub:analyse_file_structure}
As first step I do a basic structure and content analysis of the given data files on the command line in order to prepare loading the data into any tool/program\footnote{see \verb+problem1_smartfly/src/00_file_structure_analysis/file_structure_analysis.sh+ }. The main goal is to get a first impression of data quality: Do all lines of the csv file have the same number of fields? Are there any quotes within the fields that might need escaping?

% subsection analyse_file_structure (end)



\subsection{Explore the data} % (fold)
\label{sub:explore_data}

As second step I do an exploratory data analysis on the historic as well as the prediction data set. I analyse both sets to avoid potential issues arising when the prediction data is slightly (or strongly) different in structure or content from the historic data that will be used for training a model. 

I use the open source statistics software \href{http://www.r-project.org/}{R} for the exploratory data analysis since it has all necessary analysis functions and plotting tools already implemented and the package \href{http://yihui.name/knitr/}{knitr} allows for reproducible analysis.

For the exploratory data analysis I used most variables as factors since they're not really on a continuous scale (s.t. standard arithmetics would make sense)\footnote{For a list of variable types I used see \verb+problem1_smartfly/src/01_exploratory_data_analysis/resources/raw_variables.csv+}.
I iterated over the process of loading the historic data in R and looking at it in basic tables and plots until I found out which strings to mark as \verb+NA+ right from the data loading (and thus applicable to all variables), which data needs cleaning and reformatting. To get the necessary code straight without too long waiting time\footnote{The compilation of the basic plots and summary statistics for the full historic data set of 7.3 million observations took about 15 minutes per run.} I switched back and forth between the full set of historic data and a reduced set of 4000 observations (random observations not the first 4000, to get an impression of the data more close to the truth even with a reduced set).

\textbf{Assumptions} I took in these steps: 
\begin{itemize}
	\item "NA" in general and "" (empty string) for variables \verb+cancellation_code+ and \verb+tail_number+ indicate a missing value
	% \item "000000" for \verb+tail_number+ is not valid (according to Wikipedia) and is set to missing value %not used anymore since I dropped the variables with levels > 53 before the NA analysis
	\item Scheduled times either start with "00" or with "24" (I decided to remap "24" to "00")
	\item I truncated scheduled times to the hour for better intelligibility
	\item I defined the target variable \verb+is_delayed=(departure_delay > 0) & (cancelled == FALSE)+ (as per the given definition in the problem description)
\end{itemize}

The most important statistic from this exploratory analysis of historic data is for me the percentage of delayed flights (based on \verb+is_delayed+), which amounts to $28.6\%$. If I see that as a base rate, the model I estimate has to have an error rate better than this (because otherwise I could just say at random with a probability of $28.6\%$ that a flight is delayed - without any input data or model).

After I applied the same pre-processing steps (mark missing values, mark as factors, truncate times, reformat labels) I ran the same exploratory analysis on the scheduled flight data with special focus on factor levels:
\begin{itemize}
	\item The variables seem to be roughly distributed in the same manner as they are in the historic data, except for the values for \verb+year+ and \verb+month+ (which stems from the problem itself).
	% \item "0" which is as well an invalid tail number (i.e. aircraft registration number) is as well included in the scheduled flights data
	\item Not all factor variables have the same levels as in the historic flight data\footnote{Venn diagrams helped a lot to come with a clear picture of this. Installation see \url{http://www.ats.ucla.edu/stat/r/faq/venn.htm}, results see \verb+problem1_smartfly/src/01_exploratory_data_analysis/exploratory_data_analysis.pdf+}.
\end{itemize}

The difference in factor levels mattered to me since I have the experience, that when solving a classification problem with a random forest in R, the training and the prediction data need to contain the same variables with the same levels. Otherwise you can't even predict with the random forest.

More detailed output of exploratory analysis can be found at:
\begin{itemize}
	\item \verb+problem1_smartfly/src/01_exploratory_data_analysis/exploratory_data_analysis_historic.pdf+
	\item \verb+problem1_smartfly/src/01_exploratory_data_analysis/exploratory_data_analysis_scheduled.pdf+
\end{itemize}


% subsection explore_data (end)

\subsection{Prepare historic data for modeling} % (fold)
\label{sub:prepare_historic_data_for_modeling}
Independent from the model I choose it's better to remove all variables that don't help with the prediction from the data before applying an model estimation algorithm. In this case I drop the variables \verb+taxi_time_in+, \verb+taxi_time_out+, \verb+cancelled+, \verb+cancellation_code+ because they are not available in the scheduled flight data - and thus can't be used as input for delay prediction. I also drop \verb+departure_delay+ because I condensed its information into a new target variable \verb+is_delayed+ already.

Since I opted for a random forest model implemented in the R package \href{http://cran.r-project.org/web/packages/randomForest/index.html}{randomForest} I also needed to work around the issue that this implementation throws an error if factor variables with more than 53 levels are included in the data. My first \textcolor{black}{(and only)} solution to this was to exclude the concerned variables (\verb+flight_number+, \verb+tail_number+, \verb+origin_airport+, \verb+destination_airport+ - note that the scheduled time variables did not fall into this category since I truncated them to the hour).

Note that although I assume that the variable \verb+id+ does not have any prediction power regarding the delay of a flight, I don't drop it from the data since I need it for identification in the prediction stage to come up with the required order list of flight IDs. 

To work around the issue that a random forest requires the training and the prediction data to contain the same factor levels (and \verb+year+ and \verb+month+ clearly have different levels in the historic and the scheduled flight data) I transform the date and time related variables to numeric data types.

Furthermore, the algorithm for random forest does by default throw an error when encountering missing values so I check if there are missing values in any of the \textit{remaining} variables - and there are not.

More detailed output of preparation can be found at:
\begin{itemize}
	\item \verb+problem1_smartfly/src/02_prepare_data_for_modeling/prepare_data_for_modeling.pdf+
\end{itemize}


% subsection prepare_historic_data_for_modeling (end)

\subsection{Estimate and validate a model} % (fold)
\label{sub:estimate_and_validate_a_model}

I used a training sample consisting of $7\%$ (i.e. $516.206$ observations) of the historic flight data to train a random forest with default parameters:
\begin{itemize}
	\item number of variables tried at each split: \verb+mtry=floor(sqrt(ncol(df))))=3+ 
	\item number of trees in the forest: \verb+ntree=500+
\end{itemize}


\textcolor{black}{(About model selection criteria: The first model I envisioned was a logistic regression, however I could not get it to work using package glmnet in R. So I switched over to random forests in R. The reason behind both options was that I already had experience estimating these in R. Witin the model I did not select model parameters beyond the default because I did not have enough memory to run crossvalidation which would have picked the best model parameters.)}

Anyway, here is the list of variables I included for the my basic random forest estimation:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
training sample:	516206 obs. of  12 variables:
 $ id                      : chr  "3280125367763225179" "6587250861035043912" ...
 $ year                    : num  2013 2013 2013 2013 2013 ...
 $ month                   : num  8 8 8 8 8 8 8 8 8 8 ...
 $ day_of_month            : num  8 29 10 17 3 13 3 9 17 6 ...
 $ day_of_week             : num  4 4 6 6 6 2 6 5 6 2 ...
 $ scheduled_departure_time: num  7 7 12 12 15 15 18 12 12 6 ...
 $ scheduled_arrival_time  : num  8 8 14 14 17 17 19 15 15 8 ...
 $ airline                 : Factor w/ 17 levels "AA","AS","B6",..: 15 15 15 15 15 ...
 $ plane_model             : Factor w/ 6 levels "737","747","757",..: 6 6 2 1 2 3 4 ...
 $ seat_configuration      : Factor w/ 6 levels "Standard","Three Class",..: 1 3 4 ...
 $ distance_travelled      : num  185 185 508 508 329 329 213 809 809 329 ...
 $ is_delayed              : Factor w/ 2 levels "on_time","delayed": 1 1 1 1 1 1 ...
\end{verbatim}
\end{kframe}
\end{knitrout}

The final random forest has an estimated error rate of $27.97\%$ which is pretty bad given the base rate of $28.6\%$.


More detailed output of estimation, preparation for prediction and prediction can be found at:
\begin{itemize}
	\item \verb+problem1_smartfly/src/03_train_model/train_model.pdf+
	\item \verb+problem1_smartfly/src/04_prepare_data_for_prediction/prepare_data_for_prediction.pdf+
	\item \verb+problem1_smartfly/src/05_predict_delay_proba/predict_delay_proba.pdf+
\end{itemize}

% subsection estimate_and_validate_a_model (end)

\subsection{Current Limitations} % (fold)
\label{sub:current_limitations}

\begin{itemize}
	\item I only included a small percentage of the massive historic data for training 
	\item An error rate of $28\%$ of the estimated model is not acceptable.
\end{itemize}

In order to improve the model I would check out a plot of accuracy vs model complexity ("fitting graph") for train and test data set to get an impression whether the model overfits. I would double-check the result by having a look at a plot of error rate vs. training set size ("learning curve").

To be able to use more observations to train the random forest I would switch to parallel computing, first trying package \href{http://cran.r-project.org/web/packages/bigrf/index.html}{bigrf} or to a machine with more memory, but the best way forward is probably proper parallelisation.

% subsection estimate_and_validate_a_model (end)



\section{Used Software}
\label{used_software}
In general I developed on a Macbook Pro with a 2.3 GHz Intel Core i7 Processor and 16GB Memory.

\begin{itemize}
	\item git
	\item LateX
	\item R in RStudio with packages caret, ggplot, knitr, plyr, randomForest
	\item shell command line
\end{itemize}

\section{Time Spent}
\begin{itemize}
	\item 5h general setup and basic descriptive statistics
	\item 4h (failed) logistic regression
	\item 10h exploring data
	\item 12h preparing data for random forest modeling \& prediction
	\item 10h modeling and predicting data
	\item 1h wrting solution abstract 
	\item 5h attempt of cloud server setup (but the server was actually worse than my Mac)
	%\item xxh code change for parallel computation
\end{itemize}

Summing up I spend \textcolor{black}{47 hours} on this problem.


\end{document}